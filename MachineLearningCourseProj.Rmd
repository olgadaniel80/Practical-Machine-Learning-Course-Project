---
title: "Practical Machine Learning Course Project"
author: "Olga Daniel"
date: "February 8, 2019"
output: 
   
    html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
Our goal of the project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. We should create a report, answering the following questions:

    * how we built our model, 
    * how we used cross validation, 
    * what we think the expected out of sample error is, 
    * and why we made the choices we did


## Data Load And Clean

```{r}
library(caret)
library(rattle)
library(randomForest)
library(gbm)
if(!file.exists("pml-training.csv")){
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  destfile = "pml-training.csv")
}
if(!file.exists("pml-testing.csv")){
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                  destfile = "pml-testing.csv")
}

training_data <- read.csv("pml-training.csv", header = TRUE, 
                          na.strings = c("NA", "", "'#DIV/0!"))
testing_data  <- read.csv("pml-testing.csv", header = TRUE, 
                          na.strings = c("NA", "", "'#DIV/0!"))
dim(training_data)
dim(testing_data)
```

The training data include 19622 observations on 160 columns. We can see that many columns have NA or nave no values on almost every observation. We may remove them. The first seven columns give information about the people who did the test, and also timestamps. Also this data we may remove.

```{r}
# First of all we remove NA data
training_data <- training_data[, (colSums(is.na(training_data)) == 0)]
# Here we remove first seven columns with irrelevant informtion
training_data <- training_data[, -c(1:7)]
# The same manipulation on testing data
testing_data  <- testing_data[, (colSums(is.na(testing_data)) == 0)]
testing_data  <- testing_data[, -c(1:7)]
dim(training_data)
dim(testing_data)
```

## Preprocessing Data
First of all we divide our training data set into two parts - for training and for testing.
```{r}
set.seed(1234)
inTrain <- createDataPartition(y = training_data$classe, p = 0.75, list = FALSE)
training <- training_data[inTrain,]
testing <- training_data[-inTrain,]
```
We need to examine our data for missing data, skewed variables etc, so that they do not affect the prediction. In order to check the data on skewed variables, we may use histogram
```{r}
hist(as.numeric(training$classe))
mean(as.numeric(training$classe))
sd(as.numeric(training$classe))
```
As we can see, `r mean(as.numeric(training$classe))` and `r sd(as.numeric(training$classe))` are enough closed and our histogram also looks fine, without emissions.

```{r}
near_zero_var <- nearZeroVar(training, saveMetrics=TRUE)
near_zero_var
```
In the sense of missing data, we are also fine. As for strongly correlated data, we have quite a few such variables. In our example we will looking for variables with correlation coefficient more than 0.95. This is too high a coefficient, however when we selected a smaller coefficient we received too much variables. And we need to do something with them. However, within the framework of our project, we think it will be enough to simply demonstrate this stage. So, correlation coefficient 0.95. It is possible to combine them, this is desirable but not necessary.
```{r}
m <- abs(cor(training[, -53]))
diag(m) <- 0
which(m > 0.95, arr.ind = T)
```

## Model choosing
We will try different models, and choose among them the one that has the best accuracy. In order to improve the efficiency of the models we will use cross-validation with 5 folds.
```{r}
train_control <- trainControl(method="cv", number=5)
```
We start with classification tree
```{r}
mod_fit_ct <- train(classe ~ ., data = training, method = "rpart", trControl = train_control)
fancyRpartPlot(mod_fit_ct$finalModel)
# prediction
pred_ct <- predict(mod_fit_ct, testing)
conf_matrix_ct <- confusionMatrix(testing$classe, pred_ct)
conf_matrix_ct$overall[1]
```
We see that accuracy of this method is very low. The next model is random forests. 
```{r cache=TRUE}
mod_fit_rf <- train(classe ~ ., data = training, method = "rf", trControl = train_control, verbose = FALSE)
plot(mod_fit_rf)
# prediction
pred_rf <- predict(mod_fit_rf, testing)
conf_matrix_rf <- confusionMatrix(testing$classe, pred_rf)
conf_matrix_rf$overall[1]
```
Now we see much more interesting accuracy. And finally we try boosting model
```{r cache=TRUE}
mod_fit_boost <- train(classe ~ ., data = training, method = "gbm", trControl = train_control, verbose = FALSE)
plot(mod_fit_boost)
# prediction
pred_boost <- predict(mod_fit_boost, testing)
conf_matrix_boost <- confusionMatrix(testing$classe, pred_boost)
conf_matrix_boost$overall[1]
conf_matrix_boost
```

## Conclusion
As we can see, the best result was shown by random forest. In the light of the above about highly correlated data, we would like to look at the most important variables for this model, in order to consider the need to get rid of such data or the model can cope with it on its own.
```{r cache = TRUE}
varImp(mod_fit_rf)
accur <- postResample(pred_rf, testing$classe)
accur[[1]]
```
The last step is to apply our model to the test dataset.
```{r}
test_set_pred <- predict(mod_fit_rf, newdata = testing_data)
test_set_pred
```